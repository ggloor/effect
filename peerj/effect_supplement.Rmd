---
title: "Supplementary figures and code"
author: "Greg Gloor"
date: "`r format(Sys.time(), '%d %B, %Y')`"
geometry: margin=2cm
bibliography: /Users/ggloor/Library/texmf/bibtex/bib/bibdesk_refs.bib
csl: /Users/ggloor/Documents/0_git/csl_styles/bioinformatics.csl
output:
    pdf_document:
        keep_tex: false
        fig_caption: true
        toc: true
        toc_depth: 2
        dev: pdf
        includes:
            in_header: /Users/ggloor/Documents/0_git/templates/header.tex
        pandoc_args: ["-V", "classoption=onecolumn"]
---

# About this document

This document is an .Rmd document and can be found at:

github.com/ggloor/effect/effect_supplement.Rmd

The document is requires Rmarkdown and an installation of \LaTeX to work properly. It contains interspersed markdown and R code that may be compiled into a pdf document and supports the figures and assertions in the main article. R code is not exposed in the pdf document but is either referred to by `R-code.r`, or by `R-block-n`. Both are are annotated R code chunks. The former are in the supplementary `code` directory, and the latter are interspersed in the document. Both are provided so  that the interested reader can work through as need or interest arises.

## R packages required

We will need the following R packages and add-ons (`R-block-1`). The code chunk `code/setup.r` processes the input and output files for analysis, and contains pointers to the files used to create the analysis data that follows.

1. knitr (CRAN)
2. Rmarkdown (CRAN)
3. ALDEx2 (Bioconductor)
4. CoDaSeq (github.com/ggloor/CoDaSeq)
5. distEffect (github.com/ggloor/distEffect) 
5. source('code/setup.r')

```{r R-block-1, results="hide", echo=F, message=F, error=F, warnings=F}

my.path <- '~/Documents/0_git/effect/peerJ/'

# in theory - requires knitr and LaTeX installed
# from a bash prompt:
# R -e "rmarkdown::render('effect_supplement.Rmd')"
# from within an R terminal:
# rmarkdown::render('effect_supplement.Rmd')
# commands to load libraries and datasets
library(knitr)
source('~/Documents/0_git/effect/peerJ/code/setup.r') # common datasets
#source('code/plots.r')
library(distEffect)
library(ALDEx2)
source('~/Documents/0_git/effect/peerJ/code/analyze_subsets.R')

```

## Datasets

The transcriptome dataset in `data/barton_agg.tsv` is a 48-replicate, two-condition RNA-seq that was done to compare the transcriptome of the BY4741 strain (wild-type) of \emph{Saccharomyces cerevisiae} to that of a SNF2 knockout mutant strain from the same genetic background [@Gierlinski:2015aa]. The 96 samples were prepared in four batches of 24 samples. RNA was extracted from each of the biological replicates and enriched for polyadenylated RNA. ERCC external RNA spike-in mix was added to each sample [@Jiang:2011aa]. The RNA in each sample was fragmented and reverse transcribed to cDNA. The cDNA corresponding to each biological replicate was sequenced on seven lanes of an Illumina HiSeq 2000, thus giving seven technical replicates for each biological replicate. The sequencing data was downloaded from the European Nucleotide Archive under the project ID PRJEB5348. Each of the 672 fastq files, corresponding to one of the technical replicates was aligned to the complete and annotated yeast genome (NCBI project accession: PRJNA128), that had been modified to include the sequences for the 96 ERCC external RNA spike-ins (NIST SRM number: 2374), using bowtie2 v2.1.0 [@bowtie2]] with the default options. For each of the 672 replicates, the counts of sequencing reads mapped to each gene and to each of the ERCC external RNA spike-in sequences was determined using HTSeq v0.6.1 [@Anders:2015aa]. All reads with an alignment quality less than 0 were omitted. For each biological replicate, the counts for its technical replicates were aggregated by summing. Since the experiment done by [@Gierlinski:2015aa] enriched for polyadenylated RNA, counts for sequences that had been mapped to genes annotated as rRNA were removed as they were assumed to only contribute noise to the data.

The 16S rRNA gene sequencing dataset were obtained from the supplementary material of [@bian:2017] located at: https://doi.org/10.6084/m9.figshare.4535660. The two groups were extracted from the entire dataset using the extraction code in `data/effect_reproducibility_16S.R`. Samples were filtered to remove OTUs that were not observed in any sample and the count table saved in `data/tiyaini_pup_vs_ys.Rdata` for use.

For each dataset, a reference set of significant features for ALDEx2 was generated by performing and collecting 100 replicates of the entire dataset comparison using the code in `code\effect_reproducibility.R` and `code\effect_reproducibility_16S.R`, a parallel analysis of the transcriptome dataset with `edgeR` [@Anders:2013aa] was conducted using `code\effect_reproducibility_edgeR.r`. Once the reference set was collected, we conducted 100 replicates of balanced sample size comparisons for each sample size between 2 and 40 samples for the transcriptome data and between 2 and 100 samples for the 16S rRNA gene sequencing data. We collected all features that were identified as passing the significance cutoffs chosen and saved them for use.

## Reproducing the analysis

From an R command prompt you can compile this document into PDF if you have \LaTeX and pandoc installed:

`rmarkdown::render('effect_supplement.Rmd')`, or you can do the same in bash (R -e "rmarkdown::render('effect_supplement.Rmd')") or you can open the file in RStudio and compile in that environment.

\clearpage

# Effect size

Cohen's d, and similar statistics of a standardized mean difference (here-after effect size), use summary statistics to estimate the effect size [@Hedges:1985;@cohen_effect]. The standard equation is in equation 1 and broadly speaking  an  effect size of this type is the ratio of the difference $diff$ and the dispersion $disp$ of two datasets.

\begin{equation}
    z = \frac{\mu_a - \mu_b}{disp} = \frac{diff}{disp}
    \label{effect}
\end{equation}

In equation 1 all values are summary statistics from distributions $a$ and $b$, and the utility of $z$ thus depends upon how well the assumptions of each summary statistic fits the underlying distributions. If $z$ is calculated from the mean and standard deviation of a Normal distribution, then $z$ is the number of $z$ scores that separate the two mean values. 

## Effect size vs Difference

It is worth pointing out that a p-value is not a measure of magnitude of change (or difference), nor is a p-value a standard measure of effect size. Nevertheless, both p-values and difference are widely used in the high throughput sequencing literature as proxies for effect size. The most obvious example of this is when investigators use a Volcano plot which shows the relationship between p-values and  the difference between groups  [@Cui:2003aa].

Recall that one common way a p-value is calculated is from the t-statistic which has the general form shown in equation 2. Comparing equation 1 and equation 2 we see that the denominator is different; $disp$ is the denominator when calculating $z$, and the  square root of $disp$ divided by the sample size $\mathrm{N}$ is the denominator when calculating $t$. Thus, p-values are not stable estimates of effect size, but rather are strongly dependent on sample size.

\begin{equation}
    t = \frac{\mu_a - \mu_b}{\sqrt{disp / \mathrm{N}}} = \frac{diff}{\sqrt{disp / \mathrm{N}}}
    \label{ttest}
\end{equation}

## The distribution effect size ($\mathbb{E}$)

We now introduce and demonstrate the properties of the $\mathbb{E}$. The metric was first developed  and used as a convenience for meta-RNAseq [@fernandes:2013; @macklaim:2013] and later for microbiome analyses [eg. @fernandes:2014;@bian:2017], however, these publications did not investigate its properties. The purpose of $\mathbb{E}$ is to determine the standardized difference between two distributions, rather than the standardized difference between the means (or midpoints) of the distributions. Let us briefly explain the difference.

The approach taken by $\mathbb{E}$ is to calculate the median of the standardized difference of the distributions, $\vec{\mathit{eff}}$. We will use vector format and start with our two distributions represented as $\vec{a}$ and $\vec{b}$. The $\mathbb{E}$ metric is calculated as in equation \ref{Fe} from the outputs of the prior equations 3 and 4. Note that both the difference and dispersion estimates are  vectors and not point estimates.

\begin{align}
    \vec{\mathit{diff}} = \vec{a} - \vec{b}\\
    \vec{\mathit{disp}} =  max \{ \lvert \vec{a} - \boldsymbol{\rho} \vec{a}  \rvert ,\lvert \vec{b} -\boldsymbol{\rho} \vec{b} \rvert \}\\
  \vec{\mathit{eff}} =  \frac{ \vec{\mathit{diff}} }{ \vec{\mathit{disp}} }\\
  \mathbb{E} = \mathrm{med} (\vec{\mathit{eff}})
  \label{Fe}
\end{align}

In equations 3-5, we use $\mathrm{max}$ to refer to the maximum value at each position in the vector, $\mathrm{med}$ to refer to the median of the vector, $\lvert~\rvert$ to indicated the absolute value of the elements in the enclosed vector, and $\boldsymbol{\rho}$  to denote one or more random permutations of the associated vector.

Note that both the numerator and the denominator in equation 5 are  vectors as is the result since we are calculating the ratio of the vector values element-wise. The vectors recycle if one vector is different in length than the other. The numerator, $\vec{\mathit{diff}}$, is simply the signed difference between the distributions in $\vec{a}$ and $\vec{b}$, and the denominator, $\vec{\mathit{disp}}$,  is the maximum absolute difference, a novel  estimate of the pooled dispersion of the distributions. The $\vec{\mathit{disp}}$ metric is necessary since there is no vector-wise dispersion estimate in common use.

## Properties of the median of $\vec{\mathit{diff}}$, MSD

The midpoints of both $\vec{\mathit{diff}}$ and $\vec{\mathit{disp}}$ have meaning, and are calculated by ALDEx2. The median of $\vec{\mathit{diff}}$ is the median signed difference (MSD, `diff.btw` in ALDEX2)  and the median of $\vec{\mathit{disp}}$, or the median of the maximum absolute difference (MMAD), is the dispersion statistic `diff.win` returned by ALDEx2. The code contained in `R-block-2`, and Figure 1 shows the behaviour of the MSD  relative to the difference of means for three distributions as a measure of location. We can see that the two methods are essentially equivalent except in the case of a Cauchy distribution, where the difference in means clearly fails to provide an reliable estimate of location. Thus, the MSD is an efficient and safe choice to determine location regardless of the underlying distribution.

```{r R-block-2, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="Boxplots of residual values of the difference in means, or the MSD for 1000 trials between two random distributions with a known difference of 1. The number of samples in the distributions was 10, 100 or 1000. A perfect estimator of location would have a residual of 0 without any variation. The difference in means (Mean, red) and the median of the difference vector  (MSD) return very similar results except in the case of a Cauchy distribution, where the MSD is clearly preferable. The y-axis of the Cauchy distribution plot has been truncated since the limit of the difference in mean  values is very large."}
######
# how many variables are needed to converge on the real difference of 1?
# for normally distributed, cauchy and random uniform
# the mean difference is identical to the difference of means
# the Median of the vector difference is slightly less precise
# than the mean, but is robust to any distribution
######
# two functions to generate 100 random instances of different distibutions
# for plotting

b.data <- function(fun, size, disn){
    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 1)
        ab <- b-a
        data[i] <- fun(ab) -1
    }
    return(data)
}

b.data.summary <- function(fun, size, disn){
	    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 1)
        ab <- fun(b) - fun(a)
        data[i] <- ab - 1
    }
    return(data)
}

b.data.ru <- function(fun, size, disn){
    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 2)
        ab <- b-a
        data[i] <- fun(ab) - 1
    }
    return(data)
}
b.data.ru.summary <- function(fun, size, disn){
    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 2)
        ab <- fun(b) - fun(a)
        data[i] <- ab - 1
    }
    return(data)
}

par(mfrow=c(1,3))
# plot of residuals from the true difference
boxplot(list(b.data.summary(mean,10,rnorm), b.data(median,10,rnorm),
   b.data.summary(mean,100,rnorm), b.data(median,100,rnorm),
   b.data.summary(mean,1000,rnorm), b.data(median,1000,rnorm)),ylim=c(-2,2),
   main="Normal distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")
legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))

boxplot(list(b.data.summary(mean,10,rcauchy), b.data(median,10,rcauchy),
   b.data.summary(mean,100,rcauchy), b.data(median,100,rcauchy),
   b.data.summary(mean,1000,rcauchy), b.data(median,1000,rcauchy)), ylim=c(-5,5),
   main="Cauchy distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")

legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))
boxplot(list(b.data.ru.summary(mean,10,runif), b.data.ru(median,10,runif),
   b.data.ru.summary(mean,100,runif), b.data.ru(median,100,runif),
   b.data.ru.summary(mean,1000,runif), b.data.ru(median,1000,runif)), ylim=c(-1,1),
   main="Uniform distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")
legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))

```

## Properties of the median of $\vec{disp}$ (MMAD)

```{r R-block-3, eval=FALSE, echo=FALSE}
# Med(maximum absolute deviation)
# this is vectorized
# when getting the scale of one distribution x and y are the same distribution
# when getting the pooled deviation of two distributions, x and y are
# different distributions
# mmad function is distE.mmad from distEffect package

###############################
### Estimate the scaling factor
# the number of samples that are needed to achieve the
# variance precision of the normal variance
# multiple tests suggest a factor of  1.418339 +/- 9.585138e-05 is the expected
# scale. Round to 1.418 +/- 1e-4
ntest =100000
dist.size = 1000

scale.mat <-  matrix(data=NA, nrow=ntest, ncol=2)

colnames(scale.mat) = c("mmad", "sd")

for(i in 1:ntest){
a <- rnorm(dist.size, 0, 1)
scale.mat[i,1] <-distE.mmad(a,a,mid=T)
scale.mat[i,2] <- sd(a)
}
# scale factor
mean(scale.mat[,1]/scale.mat[,2])
# standard error
sd(scale.mat[,1]/scale.mat[,2])/ sqrt(nrow(scale.mat))

##############################
### Estimate efficiency
### Takes forever!

eff <- matrix(data=NA, nrow=100, ncol=2)
colnames(eff) <- c("eff_mmad", "eff_mad")
for(j in 1:100){
	ntest =1000
	dist.size = 1000

	diff.mat <-  matrix(data=NA, nrow=ntest, ncol=3)

	colnames(diff.mat) = c("mmad", "sd", "mad")

	for(i in 1:ntest){
		a <- rnorm(dist.size, 0, 1)
		diff.mat[i,1] <- distE.mmad(a,a, mid=T) / 1.4183
		diff.mat[i,2] <- sd(a)
		diff.mat[i,3] <- mad(a)
	}

	boxplot(diff.mat)
	abline(h=1, lty=2, col="grey")

	eff[j,1] <- 1/ (var(diff.mat[,1])/var(diff.mat[,2]))
	eff[j,2] <- 1/ (var(diff.mat[,3])/var(diff.mat[,2]))
}
# expected eff of mad
mean(eff[,2])
# expected eff of mmad
mean(eff[,1])

```

The MMAD is a somewhat robust, and efficient estimator of scale. `R-block-3` shows the code used to support this, but is not run for efficiency since the number of random values needed to estimate with precision is extreme.

We estimate that MMAD is $1.418 \pm 0.0001$ that of the standard deviation for a normal distribution. The efficiency of the  MMAD for estimating dispersion in a Normal distribution is 52\%, which compares favourably with that for the median absolute deviation (MAD) of 37\%. Thus, the MMAD requires, at most, double the sample size to determine dispersion at the same precision as the standard deviation. Finally, Figure 2 and `R-block-4` show the breakdown point of the MMAD for a Normal distribution. We can see that the expected value of the difference between two distributions is unchanged until 20\% of the observations in one sample are changed. This compares poorly with the MAD which has the maximum breakdown point of 50\%, but favourably with 0\% breakdown point of the standard deviation. We conclude that the MMAD and the MSD, while not perfect, are reasonable measures of dispersion and difference for a broad array of distributions.

```{r R-block-4, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="The breakdown point is the percent contamination that can be introduced into a distribution without changing the measure of scale. The breakdown point  of the MMAD is about 0.2, while the breakdown point of the standard deviation is 0. The vertical lines indicate the breakdown points."}

# Med(maximum absolute deviation) distE.mmad
# this is vectorized
# when getting the scale of one distribution x and y are the same distribution
# when getting the pooled deviation of two distributions, x and y are
# different distributions

# BREAKDOWN
# breakdown of mad is 50% as expected
# breakdown of mmad is 20% :-(, can push to 35% if use first
# quartile rather than median
# it is the maximum that is the problem here
########
dist.size = 1000

brk <- matrix(data=NA, ncol=6, nrow=dist.size)
colnames(brk) <- c("orig", "contam", "sd_orig", "sd_contam", "mad_orig", "mad_contam")

# replace values from the normal distribution with uniform values for convenience
for(i in 1:dist.size){
c <- rnorm(dist.size, 0, 1)
a <- rep(1, dist.size)
b <- a
b[1:i] <- c[1:i]

brk[i,1] <- distE.mmad(a,a,mid=T) / 1.4184
brk[i,2] <- distE.mmad(b,b,mid=T) / 1.4184
brk[i,3] <- sd(a)
brk[i,4] <- sd(b)
brk[i,5] <- mad(a)
brk[i,6] <- mad(b)
}

par(mfrow=c(1,3))
plot(brk[,2] - brk[,1], ylab ="contam - orig", xlab="N contam",
    main="MMAD", pch=19, col=rgb(0,0,0,0.2))
abline(v=200)
plot(brk[,4] - brk[,3], ylab ="contam - orig", xlab="N contam",
    main="SD", pch=19, col=rgb(0,0,0,0.2))
abline(v=1)
plot(brk[,6] - brk[,5], ylab ="contam - orig", xlab="N contam",
    main="MAD", pch=19, col=rgb(0,0,0,0.2))
abline(v=500)

```

## Comparing $\mathbb{E}$ and Cohen's d

Finally, we examine how $\mathbb{E}$ and Cohen's d compare when determining the standardized difference between two distributions. The code is shown in `R-block-5` and Figure 3 shows how the two statistics compare for Normal and Cauchy distributions, with an expected difference of 2 and a scale parameter of 1.

Figure 3 shows that as expected Cohen's d had a standardized effect size that was distributed around the mean value of 2.0 when two Normal distributions were compared. The $\mathbb{E}$ standardized effect size was distributed around a mean value of 1.4, which is expected given that the denominator of $\mathbb{E}$ is 1.418 times larger than the pooled standard deviation of 1. Applying this correction, the standardized effect size for $\mathbb{E}$ would be 2.0, the same as Cohen's d for a Normal distribution. Thus as instantiated, the $\mathbb{E}$ is less than Cohen's d, but can be easily scaled if needed. Scaling is not performed by default, since there is no guarantee that we are comparing Normal distributions in high throughput sequencing datasets.


```{r R-block-5, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="The standardized effect size of $\\mathbb{E}$ and Cohen's d (Cd) are compared for Normal and Cauchy distributions. The histograms plot one thousand random tests of the effect size for distributions of size 1000. The difference between the distributions was set at 2 with a scale parameter of 1 in each case."}
##########
# E_d vs Cohen's d for Normal and Cauchy Distribution
##########
test.out <- matrix(data=NA, nrow=1000, ncol=4)
colnames(test.out) <- c("FeN", "CdN", "FeC", "CdC")

for(i in 1:1000){
    a <- rcauchy(1000, 3, 1)
    b <- rcauchy(1000, 1, 1)

    c <- rnorm(1000, 3, 1)
    d <- rnorm(1000, 1, 1)

    test.out[i,3] <- distE.effect(a,b,mid=T)
    test.out[i,4] <- (mean(a) -mean(b)) / sqrt((var(a) + var(b))/2)
    test.out[i,1] <- distE.effect(c, d, mid=T)
    test.out[i,2] <- (mean(c) -mean(d)) / sqrt((var(c) + var(d))/2)
}

par(mfrow=c(2,2))
hist(test.out[,2], main="Normal Cd", breaks=99, xlab="Difference")
hist(test.out[,1], main='Normal Z', breaks=99, xlab="Difference")
hist(test.out[,4], main="Cauchy Cd", breaks=99, xlab="Difference")
hist(test.out[,3], main='Cauchy Z', breaks=99, xlab="Difference")
```


The situation is very different when using the standardized effect size to compare two Cauchy distributions. Here the mean effect size for Cohen's d is 0.044, or almost no effect. Figure 3 shows that this distribution is bimodal, and the median standardized effect is 0.046. However, the $\mathbb{E}$ metric gives a mean standardized effect size of 0.58 and is symmetrically distributed over a comparatively narrow range.  Recall that Cauchy distributions have very broad tails, and so a scale of 1 with a Cauchy distribution will result in a considerably broader distribution than will a Normal distribution with the same scale, and consequently the standardized effect size should be smaller than observed with a Normal distribution with the same location and scale parameters.


\clearpage

# The yeast transcriptome dataset

The main manuscript shows the results for a well-described transcriptome dataset from a SNF1 \emph{Saccharomyces cerevisae} gene knockout with a cutoff of $\mathcal{Z} > 1 \mathrm{\ or\ } 2$. Later we expand on this analysis and show a similar analyses from a 16S rRNA gene sequencing experiment in a Chinese population.

## p-value based approaches

We used the \emph{Saccharomyces cerevisia} 48-replicate SNF2 gene knockout dataset described in [@Schurch:2016aa]. We first used the `CoDaSeq R` package to remove outlier samples using a robust compositionally appropriate process [@filmozer:2009], which selects those samples that are further from all other samples than expected. The actual approach is to generate the compositionally appropriate Aitchison distance matrix after centered log-ratio transformation [@Aitchison:1986] and to identity the total distance between all samples in each group. Those samples that contribute more than twice the interquartile range to the total distance of the matrix are removed.  Using this approach we removed samples 6, \emph{10}, 13, \emph{15}, 25, \emph{31}, and 35 from the SNF2 group, leaving 41 "clean" samples, and we removed samples 21,25,28,34 and 36 from the WT group leaving 43 samples (samples that are removed by this approach but were not removed in the [@Schurch:2016aa] dataset are in italics). Sample 22 was not removed by this approach from the SNF2 group, but was using the approach in [@Gierlinski:2015aa]. In general, this approach is slightly more aggressive in removing outliers that was used in outliers than the one used in [@Gierlinski:2015aa;@Schurch:2016aa]. Discrepencies between the methods will largely occur because of the compositionally-appropriate method adopted here [@Aitchison:1986]. In the dataset used here there were 6236 genes rather than the larger number in the original report; the number of features is not expected to alter the conclusions. The code is for this step is in `code/setup.r`.


```{r R-Table-1, echo=F, results='asis'}
# example calculation of edgeR outputs
# length(which(ref.set.edgeR.et$FDR < 0.05 & abs(ref.set.edgeR.et$logFC) > 1))

#

head <- c("Tool","Method","Total genes","Significant genes")
r1 <- c("edgeR","glm","6349","4705")
r2 <- c("edgeR","et","6349","4684")
r3 <- c("edgeR","et & T>1","6349","448")
r3 <- c("edgeR","et & T>2","6349","101")
r4 <- c("ALDEx2","Wilcox","6236","4318")
r5 <- c("ALDEx2","Welch's","6236","4352")
r6 <- c("ALDEx2","$\\mathcal{Z}$ > 1","6236","2020")
r7 <- c("ALDEx2","$\\mathcal{Z}$ > 2","6236","545")

tab.data <- rbind(head,r1,r2,r3,r4,r5,r6,r7)
kable(tab.data, row.names=F, caption="TP set by method.")

```

Following the original report, we used the set of genes identified by each tool with the full set of samples as the gold standard set. Table 1 shows the number of significant genes by each method. We can see that the edgeR methods and the ALDEx2 p-value based methods return similar sets of genes at an FDR cutoff of $q < 0.05$. In fact, the different toolsets return almost the same set of genes, with 4060 of the genes being found by the four the p-value based approaches. The two $\mathbb{E}$ cutoffs return a  subset of the p-value based core set.  Note that filtering by p-value and fold-change is extraordinarily restrictive: only 101 genes pass the p-value filter and a Threshold of > 2 as defined in Gierlinski et al [-@Gierlinski:2015aa], and indeed, the Threshold of > 2 is all that is needed to identify that set of 101 genes in this dataset.


\clearpage


```{r R-Block-ty-fptp, results="hide", eval=T, echo=F, message=F, error=F, warnings=F, fig.height=8, fig.width=7, fig.cap="The  panels show the effect size distribution of features identified as false positives by $\\mathbb{E}$ at four different sample sizes in two different datasets. " }

# effect size 1, 2 fold difference
# from analyze_subsets.R
# data ref.set.yeast.Rda, test.set.yeast.Rda, ref.set.ty.Rda, ref.set.ty.Rda 
ret.list.11.ty <- get.tpfp(ref.set.ty, test.set.ty, 1, 1)
ret.list.10.ty <- get.tpfp(ref.set.ty, test.set.ty, 1, 0)
ret.list.11.yeast <- get.tpfp(ref.set.yeast, test.set.yeast, 1, 1)
ret.list.10.yeast <- get.tpfp(ref.set.yeast, test.set.yeast, 1, 0)

par(mfrow=c(2,2))
plot.FP.density(ret.list.10.yeast, ref.set.yeast, pos=c(1,4,6,8), main="Yeast: E=1, D=0", leg=T)
plot.FP.density(ret.list.11.yeast, ref.set.yeast, pos=c(1,4,6,8), main="Yeast: E=1, D=2", leg=T)
plot.FP.density(ret.list.10.ty, ref.set.ty, pos=c(1,4,6,8), main="16S: E=1, D=0", leg=T)
plot.FP.density(ret.list.11.ty, ref.set.ty, pos=c(1,4,6,8), main="16S: E=1, D=2", leg=T)

```

We can ask what is the actual effect size in the whole dataset for FP features in the subsets. Figure 4 addresses this question for both datasets and we can see the effect of including the difference cutoff. We can see that if a FP feature is identified in a subset that the feature has a strong likelihood of being actually different if the sample size is large enough. At a per-group sample size of 2, the modal effect size is close to 0, indicating that there is little discriminatory power at this sample size. This is not surprising. However, at a sample size of 5 in each group, the modal effect size of FP features abbou 0.3 in both sample sets, and increases to 0.5 with 10 samples. Importantly, at 10 samples per group the FP features all have effect sizes that are larger than 0, indicating that there is a diffenrece in the full dataset, but the difference is smaller than expected. This is consistent with the findings of Halsey et al [@Halsey:2015aa]. Thus, any FP features identified at a sample size greater than 10 are likely substantially different between the two groups, although not as different as the cutoff would suggest. 

Note however that in the small variance transcriptome dataset that a substantial fraction of the FP features have effect sizes larger than the cutoff. This can be ascribed to the FP features having larger differences  in the subsets than in the real datasets.

\clearpage

```{r R-block-ty-volcano, results="hide", eval=T, echo=F, message=F, error=F, warnings=F, fig.height=5, fig.width=12, fig.cap="Abundance  vs. Dispersion, Volcano and Effect plots. The A vs D plot shows the relationship between the dispersion in the dataset vs. the relative abundance for the yeast transcriptome dataset in the main text, and the 16S rRNA gene sequencing dataset. Dispersion is the MMAD for each feature, and the Relative abundance is the median of the clr-transformed Dir Monte-Carlo instances (rab.all and diff.win from ALDEx2). The Volcano and Effect plots show features identified by Ed , adjusted p-values and absolute differences when detecting differential features between two groups for the 16S rRNA gene sequencing dataset. These plots compare the features identified by Ed and by q-scores and a 16-fold fold-change thresholds in the full dataset. This large absolute fold change is required as the within-group dispersion is enormous in this and other 16S rRNA gene sequencing datasets. In these plots all features that have a q score less than 0.1 also have an effect size greater than 1. Thus, the features in magenta are only identified as significantly different by q scores, those in orange are significantly different by both their q score and their effect size, and features in blue are significant by their q score, their effect size and their absolute difference. The dashed grey lines in the two plots demarcate the 16-fold difference location; note that the difference is in a log2 scale. The horizontal solid line in the Volcano plot indicates a FDR (q score) of 0.1. The diagonal solid lines in the Effect plot indicate the boundary where the difference equals the dispersion; ie, where the effect size is 1." }
#### Microbiome volcano and effect plots
par(mfrow=c(1,3))
plot(ref.set.ty[[1]]$diff.win, ref.set.ty[[1]]$rab.all, pch=19,
    col=rgb(1,0,0,0.2), xlim=c(0,6), ylim=c(-11,11),
    xlab="Dispersion", ylab="Relative abundance", main="A vs D")
points(ref.set.yeast[[1]]$diff.win, ref.set.yeast[[1]]$rab.all,
    pch=19, col=rgb(0,0,1,0.2))
legend(4,-4, legend=c("16S rRNA gene", "transcriptome"),
    col=c("red", "blue"), pch=19)

p.sig <- ref.set.ty[[1]]$we.eBH < 0.1
e.sig <- abs(ref.set.ty[[1]]$effect) > 1
both.sig <- abs(ref.set.ty[[1]]$effect) > 1 & abs(ref.set.ty[[1]]$diff.btw) > 4

# volcano
plot(ref.set.ty[[1]]$diff.btw, log10(ref.set.ty[[1]]$we.eBH) * -1,
    pch=19, cex=0.8, col=rgb(0,0,0,0.1), main="Volcano",
    xlab="Difference", ylab="-1 * log(q)")
points(ref.set.ty[[1]]$diff.btw[p.sig], log10(ref.set.ty[[1]]$we.eBH[p.sig]) * -1, pch=19, cex=0.8, col=rgb(1,0,1,0.3))
points(ref.set.ty[[1]]$diff.btw[e.sig], log10(ref.set.ty[[1]]$we.eBH[e.sig]) * -1, pch=19, cex=0.8, col=rgb(0.95,0.5,0,0.3))
points(ref.set.ty[[1]]$diff.btw[both.sig], log10(ref.set.ty[[1]]$we.eBH[both.sig]) * -1, pch=19, cex=0.8, col=rgb(0,0,1,0.3))
abline(h=log10(0.1) *-1, col="grey", lwd=2)
abline(v=-4, col="grey", lty=2, lwd=2)
abline(v=4, col="grey", lty=2, lwd=2)
legend(-1,90, legend=c("not different", "q < 0.1", "E > 1", "E > 1, D>32"), pch=19,    col=c(col=rgb(0,0,0,0.5),rgb(1,0,1,0.5),col=rgb(0.95,0.5,0,0.5),col=rgb(0,0,1,0.5)))

# t(effect)
plot( ref.set.ty[[1]]$diff.win, ref.set.ty[[1]]$diff.btw,pch=19, cex=0.8, col=rgb(0,0,0,0.1),
    main="Effect",
    xlab="Dispersion", ylab="Difference")
points( ref.set.ty[[1]]$diff.win[p.sig], ref.set.ty[[1]]$diff.btw[p.sig],pch=19, cex=0.8, col=rgb(1,0,1,0.3))
points( ref.set.ty[[1]]$diff.win[e.sig], ref.set.ty[[1]]$diff.btw[e.sig],pch=19, cex=0.8, col=rgb(0.95,0.5,0,0.3))
points( ref.set.ty[[1]]$diff.win[both.sig], ref.set.ty[[1]]$diff.btw[both.sig],pch=19, cex=0.8, col=rgb(0,0,1,0.5))
abline()
abline(h=-4, col="grey", lty=2, lwd=2)
abline(h=4, col="grey", lty=2, lwd=2)
abline(0,1, col="grey", lwd=2)
abline(0,-1, col="grey", lwd=2)
```

\clearpage
# References
