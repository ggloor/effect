High throughput sequencing (HTS) datasets for transcriptomics, metagenomics and 16S rRNA gene sequencing are high dimensional, commonly conducted with pilot-scale  experiments and analyzed using a null hypothesis significance testing framework \citep{Schurch:2016aa}. Much effort has been spent identifying the best approaches and tools to determine what is `significantly different' between groups \citep{Soneson:2013,Schurch:2016aa}, but the answer seems to depend on the specific dataset and associated model parameters \citep{Thorsen:2016aa,hawinkel2017,Weiss:2017aa}. As commonly conducted the investigator determines what is `significantly different' using a null hypothesis significance approach and then decides what level of difference is `biologically meaningful' among the `significantly' different features. Graphically, this approach is  represented by the Volcano plot \citep{Cui:2003aa} where the magnitude of change (difference) is plotted vs the p-value.  One under-appreciated consequence of pilot scale research is that features with significant p-values   will have dramatically exaggerated  apparent effect sizes \citep{Halsey:2015aa}. This explains in part why so many observations  of apparent large effect fail to replicate in larger datasets \citep{Ioannidis:2005aa}. In fact, both p-values and absolute differences are poor predictors of which differences would reproduce if the experiment were conducted again \citep{Cumming:2008aa,Halsey:2015aa}


%In part, the irreproducibility of p-value based hypothesis tests is due to the low statistical power of pilot-scale experimental designs coupled with the very large number of features (genes, transcripts, etc.) used for testing.  Paradoxically, when power is low, any features (genes, transcripts, etc) that are observed to be `significantly different' must have an inflated effect size \citep{Halsey:2015aa}.  Thus, `significant' features are expected to regress to their true difference between groups when the experiment is replicated, and as usually happens if the replication set is itself underpowered, replication is therefore unlikely to identify the same set of significant features.

%However, taking inter-sample variation into account by plotting the standardized effect size vs. p-values results in a much better fit \citep{gloor:effect}. 

As many have pointed out, p-values are not useful proxies for biological relevance since p-values are designed, colloquially speaking, to estimate the likelihood of no true difference; p-values are not a test that the alternate hypothesis is true. They can only be used to estimate false-discovery rates if the p-value is calculated from a distribution that is appropriate for the experimental data, if reasonable estimates for the statistical power exist and a if reasonable estimate of the a priori probability that the null hypothesis is false \citep{Colquhoun:2014aa, Halsey:2015aa}. Simply put, p-values can only be used to test \emph{if} there is no difference between groups, not to measure the \emph{magnitude} of change between groups \citep{coe2002s,Colquhoun:2014aa}. The tension between the information that p-values provide and what the investigator needs is why magnitude of change cutoffs \citep{Cui:2003aa}, or other ad-hoc methods are used when deciding what is biologically relevant. Null-hypothesis significance based testing methods  also have the property that the number of significant features identified  is affected by the number of samples being compared. Thus leads to the concept of statistical power, where the experiment is designed such that statistical power is prioritized over biological significance. 

On the other hand, a standardized effect size addresses the issues of interest to the biologist:``what is reproducibly different?'' or ``would I identify the same true positive features as differential if the experiment were repeated?"  \citep{coe2002s,shinichi:2004,Colquhoun:2014aa,gloor:effect}. Standardized effect size statistics start from the assumption that there is a difference, but that the difference can be arbitrarily close to zero. Unfortunately,  standardized effect size metrics are not routinely used when analyzing HTS datasets, and one potential barrier to their use is that parametric effect size statistics may not be suitable for  HTS datasets.  

Here we introduce a simple non-parametric standardized effect size statistic for distributions, $\mathcal{E}_{d}$, that is implemented in the ALDEx2, omicplotR  and CoDaSeq R packages.  The $\mathcal{E}_{d}$ statistic has been used in both meta-transcriptome and microbiome studies, for example see \citep{macklaim:2013, bian:2017}, and has been shown to give remarkably reproducible results even with extremely small sample sizes \citep{nelson:2015vaginal}.   $\mathcal{E}_{d}$  has a near monotonic relationship with p-values (Supplement Figure 1). However, it is unknown how  $\mathcal{E}_{d}$  compares with  p-values or other effect size estimates, how many samples are required, and its sensitivity and specificity characteristics. 
%\enlargethispage{12pt}

