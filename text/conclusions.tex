By default, we want to know both `what is significant' and `what is different' \citep{Cui:2003aa}.  Both of these questions can be addressed with a standardized effect size statistic that scales the difference between features by their dispersion. We have found plots of difference and dispersion to be an exceeding useful tool when examining HTS datasets \citep{gloor:effect}. Furthermore, datasets analyzed by this approach have proven to be remarkably reproducible as shown by independent lab validation \citep{macklaim:2013, nelson:2015vaginal}

The $\mathcal{E}_{d}$ statistic outlined here is a relatively robust statistic with the attractive property that it consistently  identifies almost all the same set of true features regardless of the underlying distribution as shown in Figure \ref{fig:01}, and the number of samples as shown in Figure \ref{fig:02}. In marked contrast, even the best p-value based  approaches can identify only a small proportion of the features at small samples sizes that would have been found in the full dataset \citep{Schurch:2016aa}. Thus, the simple metric outlined here  can correctly identify the `true positive' set even when the number of samples is very small. Note that fold-change thresholds as is commonly used, is not the same as an standardized effect statistic, and applying the threshold values of \citep{Schurch:2016aa} while reducing the features that are found does not necessarily enhance reproducibility (Figures \ref{fig:02} and \ref{fig:03} ). 

The tradeoff when using the $\mathcal{E}_{d}$ statistic is that at very low sample sizes the False Discovery Rate can be extreme; in this dataset and with and with a cutoff of $\mathcal{E}_{d} > 1$, the FDR is 40\% with two samples, but falls to less than 10\% only when there are 15 or more samples. Adding in an absolute fold-change restriction reduces the FDR substantially. Further tempering this, is the observation that the false positive features are frequently  close to the cutoff in the full dataset: that is, false positive features typically are true positives at slightly lower effect sizes or absolute fold changes. This is in contrast to the well-known random uniform distribution of false positive p-values. The Supplement shows additional evidence that the  $\mathcal{E}_{d}$ statistic is generally useful, having essentially the same characteristics in a 16S rRNA gene sequencing dataset which has much larger per feature dispersion. 

This work  describes the  $\mathcal{E}_{d}$ statistic for examining high throughput sequencing datasets. The statistic is relatively robust and efficient, and answers the question most desired by the biologist, namely `what is reproducibly different'.   $\mathcal{E}_{d}$ is computed in the ALDEx2 R package as the `effect' output where it is the median of the inferred technical and biological data, and in the CoDaSeq R package where it acts only the point estimates of the data. Interactive exploration of effect sizes can be done in the omicplotR Bioconductor package \citep{omicplot}.
