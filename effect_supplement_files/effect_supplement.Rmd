---
title: "Supplementary figures and code"
author: "Greg Gloor"
date: "`r format(Sys.time(), '%d %B, %Y')`"
geometry: margin=2cm
bibliography: bibdesk_refs.bib
csl: /Users/ggloor/Documents/0_git/csl_styles/bioinformatics.csl
output:
    pdf_document:
        keep_tex: false
        fig_caption: true
        toc: true
        toc_depth: 2
        dev: pdf
        includes:
            in_header: /Users/ggloor/Documents/0_git/templates/header.tex
        pandoc_args: ["-V", "classoption=onecolumn"]
---

# About this document

This document is an .Rmd document and can be found at:

github.com/ggloor/effect/effect_supplement.Rmd

The document is requires Rmarkdown and an installation of \LaTeX to work properly. It contains interspersed markdown and R code that may be compiled into a pdf document and supports the figures and assertions in the main article. R code is not exposed in the pdf document but is either referred to by `R-code.r`, or by `R-block-n`. Both are are annotated R code chunks. The former are in the supplementary `code` directory, and the latter are interspersed in the document. Both are provided so  that the interested reader can work through as need or interest arises.

## R packages required

We will need the following R packages and add-ons (`R-block-1`). The code chunk `code/setup.r` processes the input and output files for analysis, and contains pointers to the files used to create the analysis data that follows.

1. knitr (CRAN)
2. Rmarkdown (CRAN)
3. ALDEx2 (Bioconductor)
4. CoDaSeq (github.com/ggloor/CoDaSeq)
5. source('code/setup.r')

```{r R-block-1, results="hide", echo=F, message=F, error=F, warnings=F}

# in theory - requires knitr and LaTeX installed
# from a bash prompt:
# R -e "rmarkdown::render('effect_supplement.Rmd')"
# from within an R terminal:
# rmarkdown::render('effect_supplement.Rmd')
# commands to load libraries and datasets
library(knitr)
source('code/setup.r') # common datasets
#source('code/plots.r')
source('code/analyze_subsets.R')
```

## Datasets

The transcriptome dataset in `data/barton_agg.tsv` is a 48-replicate, two-condition RNA-seq that was done to compare the transcriptome of the BY4741 strain (wild-type) of \emph{Saccharomyces cerevisiae} to that of a SNF2 knockout mutant strain from the same genetic background [@Gierlinski:2015aa]. The 96 samples were prepared in four batches of 24 samples. RNA was extracted from each of the biological replicates and enriched for polyadenylated RNA. ERCC external RNA spike-in mix was added to each sample [@Jiang:2011aa]. The RNA in each sample was fragmented and reverse transcribed to cDNA. The cDNA corresponding to each biological replicate was sequenced on seven lanes of an Illumina HiSeq 2000, thus giving seven technical replicates for each biological replicate. The sequencing data was downloaded from the European Nucleotide Archive under the project ID PRJEB5348. Each of the 672 fastq files, corresponding to one of the technical replicates was aligned to the complete and annotated yeast genome (NCBI project accession: PRJNA128), that had been modified to include the sequences for the 96 ERCC external RNA spike-ins (NIST SRM number: 2374), using bowtie2 v2.1.0 [@bowtie2]] with the default options. For each of the 672 replicates, the counts of sequencing reads mapped to each gene and to each of the ERCC external RNA spike-in sequences was determined using HTSeq v0.6.1 [@Anders:2015aa]. All reads with an alignment quality less than 0 were omitted. For each biological replicate, the counts for its technical replicates were aggregated by summing. Since the experiment done by [@Gierlinski:2015aa] enriched for polyadenylated RNA, counts for sequences that had been mapped to genes annotated as rRNA were removed as they were assumed to only contribute noise to the data.

The 16S rRNA gene sequencing dataset were obtained from the supplementary material of [@bian:2017] located at: https://doi.org/10.6084/m9.figshare.4535660. The two groups were extracted from the entire dataset using the extraction code in `data/effect_reproducibility_16S.R`. Samples were filtered to remove OTUs that were not observed in any sample and the count table saved in `data/tiyaini_pup_vs_ys.Rdata` for use.

For each dataset, a reference set of significant features for ALDEx2 was generated by performing and collecting 100 replicates of the entire dataset comparison using the code in `code\effect_reproducibility.R` and `code\effect_reproducibility_16S.R`, a parallel analysis of the transcriptome dataset with `edgeR` [@Anders:2013aa] was conducted using `code\effect_reproducibility_edgeR.r`. Once the reference set was collected, we conducted 100 replicates of balanced sample size comparisons for each sample size between 2 and 40 samples for the transcriptome data and between 2 and 100 samples for the 16S rRNA gene sequencing data. We collected all features that were identified as passing the significance cutoffs chosen and saved them for use.

## Reproducing the analysis

From an R command prompt you can compile this document into PDF if you have \LaTeX and pandoc installed:

`rmarkdown::render('effect_supplement.Rmd')`, or you can do the same in bash (R -e "rmarkdown::render('effect_supplement.Rmd')") or you can open the file in RStudio and compile in that environment.

\clearpage

# Effect size

Cohen's d, and similar statistics of a standardized mean difference (here-after effect size), use summary statistics to estimate the effect size [@Hedges:1985;@cohen_effect]. The standard equation is in equation 1 and broadly speaking  an  effect size of this type is the ratio of the difference $\delta$ and the dispersion $\sigma$ of two datasets.

\begin{equation}
    \theta = \frac{\mu_a - \mu_b}{\sigma} = \frac{\delta}{\sigma}
    \label{effect}
\end{equation}

In equation 1 all values are summary statistics from distributions A and B, and the utility of $\theta$ thus depends upon how well the assumptions of each summary statistic fits the underlying distributions.

## Effect size vs Difference

```{r volcvseff, echo=F, fig.height=3, fig.width=7, fig.cap="The relationship between a p-value and the elements that are used to calculate the p-value, $\\delta$ and $\\sigma$ are shown in the left two panels labelled Volcano and Geyser. The relationship between a p-value and $\\theta$ is in the third panel, labelled Mountain. The arrow locates the feature with the largest magnitude of change, and the square bracket locates the feature with the largest effect size and lowest p-value."}
par(mfrow=c(1,3), cex.lab=1.5)
plot(ref.set.ty[[1]]$diff.btw, ref.set.ty[[1]]$we.ep + 1e-310, pch=19, col=rgb(0,0,0,0.1), log="y", main="Volcano", xlab=expression(delta), ylab="p-value")
text(-7.5, 1e-61, labels="<")
text(-3.9, 1e-99, labels="]")
plot(ref.set.ty[[1]]$diff.win, ref.set.ty[[1]]$we.ep + 1e-310, pch=19, col=rgb(0,0,0,0.1), log="y", main="Geyser", xlab=expression(sigma), ylab="p-value")
text(4.9, 1e-61, labels="<")
text(2, 1e-99, labels="]")
plot(ref.set.ty[[1]]$effect, ref.set.ty[[1]]$we.ep + 1e-310, pch=19, col=rgb(0,0,0,0.1), log="y", main="Mountain", xlab="Ed", ylab="p-value")
text(-1.5, 1e-61, labels="<")
text(-2.1, 1e-99, labels="]")
```

It is worth pointing out that a p-value is not a measure of magnitude of change (or difference), nor is a p-value a measure of effect size. Nevertheless, both p-values and difference are widely used in the high throughput sequencing literature as proxies for effect size. The most obvious example of this is when investigators use a Volcano plot which shows the relationship between p-values and  the difference between groups  [@Cui:2003aa].

Recall that one common way a p-value is calculated is from the t-statistic which has the general form shown in equation 2. Comparing equation 1 and equation 2 we see that the denominator is different; $\sigma$ is the denominator when calculating $\theta$, and the  square root of $\sigma$ divided by the sample size $\mathrm{N}$ is the denominator when calculating $t$. Thus, p-values are not stable estimates of effect size, but rather are strongly dependent on sample size.

\begin{equation}
    t = \frac{\mu_a - \mu_b}{\sqrt{\sigma / \mathrm{N}}} = \frac{\delta}{\sqrt{\sigma / \mathrm{N}}}
    \label{ttest}
\end{equation}


The Volcano plot in Figure 1 shows the relationship between p-values and $\sigma$ or $\delta$, and between p-values and $\theta$. We see that the relationship between p-values and $\delta$ (difference) appears to be relatively strong in the Volcano plot. However, note that there is considerable scatter around the relationship, such that the feature with the largest difference (denoted by the <), does not have the lowest p-value. This is troubling, and the reason for this becomes evident when we examine the 'Geyser' plot which shows that this feature has a very large dispersion. Finally, we can see in the 'Mountain' plot that the feature has a large, but not the largest standardized mean difference. Conversely, the feature with the lowest p-value (denoted by the ]) does not have the largest $\delta$, but has a combination of a large $\delta$ and a small $\sigma$. This is the information we actually want to determine, and we can clearly see that this feature has the largest absolute effect size.



## The distribution effect size ($\mathcal{E}_{d}$)

We now demonstrate the properties of the $\mathcal{E}_{d}$. The metric was first developed  and used as a convenience for meta-RNAseq [@fernandes:2013; @macklaim:2013] and later for microbiome analyses [eg. @fernandes:2014;@bian:2017], however, these publications did not investigate its properties. The purpose of $\mathcal{E}_{d}$ is to determine the standardized difference between two distributions, rather than the standardized difference between the means (or midpoints) of the distributions. Let us briefly explain the difference.

The approach taken by $\mathcal{E}_{d}$ is to calculate the median of the standardized difference of the distributions, $\vec{\varepsilon}$. We will use vector format and start with our two distributions represented as $\vec{a}$ and $\vec{b}$. The $\mathcal{E}_{d}$ metric is calculated as in equation \ref{Fe} from the outputs of the prior equations 3 and 4. Note that both the difference and dispersion estimates are  vectors and not point estimates.

\begin{align}
    \vec{\delta} = \vec{a} - \vec{b}\\
    \vec{\sigma} =  max \{ \lvert \vec{A} - \boldsymbol{\rho} \vec{A}  \rvert ,\lvert \vec{B} -\boldsymbol{\rho} \vec{B} \rvert \}\\
  \vec{\varepsilon} = med \bigg\{ \frac{ \vec{\delta} }{ \vec{\sigma} }\bigg\}
  \label{Fe}
\end{align}

In equations 3-5, we use $max$ to refer to the maximum value at each position in the vector, $med$ to refer to the median of the vector, $\lvert~\rvert$ to indicated the absolute value of the elements in the enclosed vector, and $\boldsymbol{\rho}$  to denote one or more random permutations of the associated vector.

Note that both the numerator and the denominator in equation 5 are  vectors, and that consequently we are calculating the ratio of the values element-wise. The numerator, $\vec{\delta}$, is simply the signed difference between the distributions in $\vec{a}$ and $\vec{b}$, and the denominator, $\vec{\sigma}$,  is the maximum absolute difference, a novel  estimate of the pooled dispersion of the distributions. The $\vec{\sigma}$ metric is necessary since there is no vector-wise dispersion estimate in common use.

## Properties of the median of $\vec{\delta}$

The midpoints of both $\vec{\delta}$ and $\vec{\sigma}$ have meaning, and are calculated by ALDEx2. The median of $\vec{\delta}$ is the median signed difference (MSD, `diff.btw` in ALDEX2)  and the median of $\vec{\sigma}$, or the median of the maximum absolute difference (MMAD), is the dispersion statistic `diff.win` returned by ALDEx2. The code contained in `R-block-2`, and Figure 2 shows the behaviour of the MSD  relative to the difference of means for three distributions as a measure of location. We can see that the two methods are essentially equivalent except in the case of a Cauchy distribution, where the difference in means clearly fails to provide an reliable estimate of location. Thus, the median of $\vec{\delta}$ is an efficient and safe choice to determine location regardless of the underlying distribution.

```{r R-block-2, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="Boxplots of residual values of the difference in means, or the median of $\\vec{\\delta}$ (MSD) for 1000 trials between two random distributions with a known difference of 1. The number of samples in the distributions was 10, 100 or 1000. A perfect estimator of location would have a residual of 0 without any variation. The difference in means (Mean, red) and the median of the difference vector  (MSD) return very similar results except in the case of a Cauchy distribution, where the MSD is clearly preferable. The y-axis of the Cauchy distribution plot has been truncated since the limit of the difference in mean  values is very large."}
######
# how many variables are needed to converge on the real difference of 1?
# for normally distributed, cauchy and random uniform
# the mean difference is identical to the difference of means
# the Median of the vector difference is slightly less precise
# than the mean, but is robust to any distribution
######
# two functions to generate 100 random instances of different distibutions
# for plotting

b.data <- function(fun, size, disn){
    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 1)
        ab <- b-a
        data[i] <- fun(ab) -1
    }
    return(data)
}
b.data.ru <- function(fun, size, disn){
    data <- vector()
        for(i in 1:1000){
        a <- disn(size, 0, 1)
        b <- disn(size, 1, 2)
        ab <- b-a
        data[i] <- fun(ab) - 1
    }
    return(data)
}

par(mfrow=c(1,3))
# plot of residuals from the true difference
boxplot(list(b.data(mean,10,rnorm), b.data(median,10,rnorm),
   b.data(median,100,rnorm), b.data(median,100,rnorm),
   b.data(median,1000,rnorm), b.data(median,1000,rnorm)),ylim=c(-2,2),
   main="Normal distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")
legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))

boxplot(list(b.data(mean,10,rcauchy), b.data(median,10,rcauchy),
   b.data(mean,100,rcauchy), b.data(median,100,rcauchy),
   b.data(mean,1000,rcauchy), b.data(median,1000,rcauchy)), ylim=c(-5,5),
   main="Cauchy distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")

legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))
boxplot(list(b.data.ru(mean,10,runif), b.data.ru(median,10,runif),
   b.data.ru(mean,100,runif), b.data.ru(median,100,runif),
   b.data.ru(mean,1000,runif), b.data.ru(median,1000,runif)), ylim=c(-1,1),
   main="Uniform distribution", names=c("10", "10", "100", "100",
   "1000", "1000"), col=c("red", "blue", "red","blue","red","blue"),
   border=c("red", "blue", "red","blue","red","blue"))
abline(h=0, lty=2, col="grey")
legend("topright",legend=c("Mean", "MSD"), pch=18, col=c("red","blue"))

```

## Properties of the median of $\vec{\sigma}$ (MMAD)

```{r R-block-3, eval=FALSE, echo=FALSE}
# Med(maximum absolute deviation)
# this is vectorized
# when getting the scale of one distribution x and y are the same distribution
# when getting the pooled deviation of two distributions, x and y are
# different distributions
mmad <- function(x,y){
  pmax(abs(x - sample(x)), abs(y - sample(y)))
}
###############################
### Estimate the scaling factor
# the number of samples that are needed to achieve the
# variance precision of the normal variance
# multiple tests suggest a factor of  1.418339 +/- 9.585138e-05 is the expected
# scale. Round to 1.418 +/- 1e-4
ntest =100000
dist.size = 1000

scale.mat <-  matrix(data=NA, nrow=ntest, ncol=2)

colnames(scale.mat) = c("mmad", "sd")

for(i in 1:ntest){
a <- rnorm(dist.size, 0, 1)
scale.mat[i,1] <- median(mmad(a,a))
scale.mat[i,2] <- sd(a)
}
# scale factor
mean(scale.mat[,1]/scale.mat[,2])
# standard error
sd(scale.mat[,1]/scale.mat[,2])/ sqrt(nrow(scale.mat))

##############################
### Estimate efficiency
eff <- matrix(data=NA, nrow=100, ncol=2)
colnames(eff) <- c("eff_mmad", "eff_mad")
for(j in 1:100){
ntest =1000
dist.size = 1000

diff.mat <-  matrix(data=NA, nrow=ntest, ncol=3)

colnames(diff.mat) = c("mmad", "sd", "mad")

for(i in 1:ntest){
a <- rnorm(dist.size, 0, 1)
diff.mat[i,1] <- median(mmad(a,a)) / 1.4183
diff.mat[i,2] <- sd(a)
diff.mat[i,3] <- mad(a)
}
}
boxplot(diff.mat)
abline(h=1, lty=2, col="grey")

eff[j,1] <- 1/ (var(diff.mat[,1])/var(diff.mat[,2]))
eff[j,2] <- 1/ (var(diff.mat[,3])/var(diff.mat[,2]))

# expected eff of mad
mean(eff[,2])
# expected eff of mmad
mean(eff[,1])

```

The MMAD is a somewhat robust, and efficient estimator of scale. `R-block-3` shows the code used to support this, but is not run for efficiency since the number of random values needed to estimate with precision is extreme.

We estimate that MMAD is $1.418 \pm 0.0001$ that of the standard deviation for a normal distribution. The efficiency of the  MMAD for estimating dispersion in a Normal distribution is 52\%, which compares favourably with that for the median absolute deviation (MAD) of 37\%. Thus, the MMAD requires, at most, double the sample size to determine dispersion at the same precision as the standard deviation. Finally, Figure 3 and `R-block-4` show the breakdown point of the MMAD for a Normal distribution. We can see that the expected value of the difference between two distributions is unchanged until 20\% of the observations in one sample are changed. This compares poorly with the MAD which has the maximum breakdown point of 50\%, but favourably with 0\% breakdown point of the standard deviation. We conclude that the MMAD and the median of $\vec{\delta}$, while not perfect, are reasonable measures of dispersion and difference for a broad array of distributions.

```{r R-block-4, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="The breakdown point is the percent contamination that can be introduced into a distribution without changing the measure of scale. The breakdown point  of the MMAD is about 0.2, while the breakdown point of the standard deviation is 0. The vertical lines indicate the breakdown points."}

# Med(maximum absolute deviation)
# this is vectorized
# when getting the scale of one distribution x and y are the same distribution
# when getting the pooled deviation of two distributions, x and y are
# different distributions
mmad <- function(x,y){
  pmax(abs(x - sample(x)), abs(y - sample(y)))
}

# BREAKDOWN
# breakdown of mad is 50% as expected
# breakdown of mmad is 20% :-(, can push to 35% if use first
# quartile rather than median
# it is the maximum that is the problem here
########
dist.size = 1000

brk <- matrix(data=NA, ncol=6, nrow=dist.size)
colnames(brk) <- c("orig", "contam", "sd_orig", "sd_contam", "mad_orig", "mad_contam")

# replace values from the normal distribution with uniform values for convenience
for(i in 1:dist.size){
c <- rnorm(dist.size, 0, 1)
a <- rep(1, dist.size)
b <- a
b[1:i] <- c[1:i]

brk[i,1] <- median(mmad(a,a)) / 1.4184
brk[i,2] <- median(mmad(b,b)) / 1.4184
brk[i,3] <- sd(a)
brk[i,4] <- sd(b)
brk[i,5] <- mad(a)
brk[i,6] <- mad(b)
}

par(mfrow=c(1,2))
plot(brk[,2] - brk[1], ylab ="contam - orig", xlab="N contam",
    main="MMAD", pch=19, col=rgb(0,0,0,0.2))
abline(v=200)
plot(brk[,4] - brk[3], ylab ="contam - orig", xlab="N contam",
    main="SD", pch=19, col=rgb(0,0,0,0.2))
abline(v=1)

```

## Comparing $\mathcal{E}_{d}$ and Cohen's d

Finally, we examine how $\mathcal{E}_{d}$ and Cohen's d compare when determining the standardized difference between two distributions. The code is shown in `R-block-5` and Figure 4 shows how the two statistics compare for Normal and Cauchy distributions, with an expected difference of 2 and a scale parameter of 1.

Figure 4 shows that as expected Cohen's d had a standardized effect size that was distributed around the mean value of 2.0 when two Normal distributions were compared. The $\mathcal{E}_{d}$ standardized effect size was distributed around a mean value of 1.4, which is expected given that the denominator of $\mathcal{E}_{d}$ is 1.418 times larger than the pooled standard deviation of 1. Applying this correction, the standardized effect size for $\mathcal{E}_{d}$ would be 2.0, the same as Cohen's d for a Normal distribution. Thus as instantiated, the $\mathcal{E}_{d}$ is more conservative than Cohen's d, but can be easily scaled if needed. Scaling is not performed by default, since there is no guarantee that we are comparing Normal distributions in high throughput sequencing datasets.


```{r R-block-5, results="hide", eval=TRUE, echo=F, message=F, error=F, warnings=F, fig.cap="The standardized effect size of $\\mathcal{E}_{d}$ and Cohen's d (Cd) are compared for Normal and Cauchy distributions. The histograms plot one thousand random tests of the effect size for distributions of size 1000. The difference between the distributions was set at 2 with a scale parameter of 1 in each case."}
##########
# E_d vs Cohen's d for Normal and Cauchy Distribution
##########
test.out <- matrix(data=NA, nrow=1000, ncol=4)
colnames(test.out) <- c("FeN", "CdN", "FeC", "CdC")

for(i in 1:1000){
    a <- rcauchy(1000, 3, 1)
    b <- rcauchy(1000, 1, 1)

    c <- rnorm(1000, 3, 1)
    d <- rnorm(1000, 1, 1)

    test.out[i,3] <- median( (a - b)/ (mmad(a,b)) )
    test.out[i,4] <- (mean(a) -mean(b)) / sqrt((var(a) + var(b))/2)
    test.out[i,1] <- median( (c - d)/ (mmad(c,d)) )
    test.out[i,2] <- (mean(c) -mean(d)) / sqrt((var(c) + var(d))/2)
}
par(mfrow=c(2,2))
hist(test.out[,2], main="Normal Cd", breaks=99, xlab="Difference")
hist(test.out[,1], main=expression('Normal d'[NEF]), breaks=99, xlab="Difference")
hist(test.out[,4], main="Cauchy Cd", breaks=99, xlab="Difference")
hist(test.out[,3], main=expression('Cauchy d'[NEF]), breaks=99, xlab="Difference")
```


The situation is very different when using the standardized effect size to compare two Cauchy distributions. Here the mean effect size for Cohen's d is 0.044, or almost no effect. Figure 4 shows that this distribution is bimodal, and the median standardized effect is 0.046. However, the $\mathcal{E}_{d}$ metric gives a mean standardized effect size of 0.58 and is symmetrically distributed over a comparatively narrow range.  Recall that Cauchy distributions have very broad tails, and so a scale of 1 with a Cauchy distribution will result in a considerably broader distribution than will a Normal distribution with the same scale, and consequently the standardized effect size should be smaller than observed with a Normal distribution with the same location and scale parameters.

\clearpage

## The NULL model

Figure 5 and associated code in `R-Block-dist` show the underlying distributions used in Figure 1 in the main paper. This figure shows the sahpe of the distributions used to determine the NULL model graphs. We show here a single random instance of the 40 x 40 comparison where the estimated $\mathcal{E}_{d}$ value is 1 in a Normal distribution.

```{r R-Block-dist, results="hide", eval=T, echo=F, message=F, error=F, warnings=F, fig.height=8, fig.width=8,  fig.cap="Example distributions. Random samples of size 40 were drawn from Normal (N), Uniform (U), Cauchy (C) or Beta (B) distributions. Distributions were either the base distribution (0), or perturbed by a small amount (1). Distribtions pairs are color coded using the same colors as in Figure 1 in the main manuscript."}
## dataset
set.seed(4)
rn0 <- rnorm(40)
rn1 <- rnorm(40, mean=1)

rc0 <- rcauchy(40)
rc1 <- rcauchy(40, location=2.2)

ru0 <- runif(40, min=0, max=1) * 4
ru1 <- runif(40, min=0.33,max=1.33) * 4

rb0 <- rbeta(40, 1,5) * 4
rb1 <- (rbeta(40, 1,5) + .166) * 4

r.list <- list(rn0, rn1, ru0, ru1, rc0, rc1, rb0,rb1)
par(mfrow=c(1,1))
# plot
stripchart(r.list, ylim=c(-4,6), vertical =T, method="jitter", pch=19,
   col=c(rgb(0,0,0,0.2),rgb(0,0,0,0.2),
         rgb(1,0,0,0.2),rgb(1,0,0,0.2),
         rgb(.9,.6,0,0.2),rgb(.9,.6,0,0.2),
         rgb(0,1,1,0.2),rgb(0,1,1,0.2) ),
    at=c(1.25,1.75,3.25,3.75,5.25,5.75,7.25,7.75),
    group.names=c("N0","N1","U0","U1","C0","C1","B0","B1"))


```

\clearpage

# The yeast transcriptome dataset

The main manuscript shows the results for a well-described transcriptome dataset from a SNF1 \emph{Saccharomyces cerevisae} gene knockout with a cutoff of $\mathcal{E}_{d} > 1 \mathrm{\ or\ } 2$. Later we expand on this analysis and show a similar analyses from a 16S rRNA gene sequencing experiment in a Chinese population.

## p-value based approaches

We used the \emph{Saccharomyces cerevisia} 48-replicate SNF2 gene knockout dataset described in [@Schurch:2016aa]. We first used the `CoDaSeq R` package to remove outlier samples using a robust compositionally appropriate process [@filmozer:2009], which selects those samples that are further from all other samples than expected. The actual approach is to generate the compositionally appropriate Aitchison distance matrix after centered log-ratio transformation [@Aitchison:1986] and to identity the total distance between all samples in each group. Those samples that contribute more than twice the interquartile range to the total distance of the matrix are removed.  Using this approach we removed samples 6, \emph{10}, 13, \emph{15}, 25, \emph{31}, and 35 from the SNF2 group, leaving 41 "clean" samples, and we removed samples 21,25,28,34 and 36 from the WT group leaving 43 samples (samples that are removed by this approach but were not removed in the [@Schurch:2016aa] dataset are in italics). Sample 22 was not removed by this approach from the SNF2 group, but was using the approach in [@Gierlinski:2015aa]. In general, this approach is slightly more aggressive in removing outliers that was used in outliers than the one used in [@Gierlinski:2015aa;@Schurch:2016aa]. Discrepencies between the methods will largely occur because of the compositionally-appropriate method adopted here [@Aitchison:1986]. In the dataset used here there were 6236 genes rather than the larger number in the original report; the number of features is not expected to alter the conclusions. The code is for this step is in `code/setup.r`.


```{r R-Table-1, echo=F, results='asis'}
# example calculation of edgeR outputs
# length(which(ref.set.edgeR.et$FDR < 0.05 & abs(ref.set.edgeR.et$logFC) > 1))

#

head <- c("Tool","Method","Total genes","Significant genes")
r1 <- c("edgeR","glm","6349","4705")
r2 <- c("edgeR","et","6349","4684")
r3 <- c("edgeR","et & T>1","6349","448")
r3 <- c("edgeR","et & T>2","6349","101")
r4 <- c("ALDEx2","Wilcox","6236","4318")
r5 <- c("ALDEx2","Welch's","6236","4352")
r6 <- c("ALDEx2","$\\mathcal{E}_{d}$ > 1","6236","2020")
r7 <- c("ALDEx2","$\\mathcal{E}_{d}$ > 2","6236","545")

tab.data <- rbind(head,r1,r2,r3,r4,r5,r6,r7)
kable(tab.data, row.names=F, caption="TP set by method.")

```

Following the original report, we used the set of genes identified by each tool with the full set of samples as the gold standard set. Table 1 shows the number of significant genes by each method. We can see that the edgeR methods and the ALDEx2 p-value based methods return similar sets of genes at an FDR cutoff of $q < 0.05$. In fact, the different toolsets return almost the same set of genes, with 4060 of the genes being found by the four the p-value based approaches. The two $\mathcal{E}_{d}$ cutoffs return a  subset of the p-value based core set.  Note that filtering by p-value and fold-change is extraordinarily restrictive: only 101 genes pass the p-value filter and a Threshold of > 2 as defined in Gierlinski et al [-@Gierlinski:2015aa], and indeed, the Threshold of > 2 is all that is needed to identify that set of 101 genes in this dataset.


\clearpage


```{r R-Block-ty-fptp, results="hide", eval=T, echo=F, message=F, error=F, warnings=F, fig.height=9, fig.width=7, fig.cap="Comparing Ed and adjusted p-values when detecting differential features between two groups in a 16S rRNA gene sequencing dataset. The top four panels show the median and 95% confidence interval of the true positive rate (TPR) and false positive rate (FPR) at different cutoff values. The top two panels show the values for the q-value, the Benjamini-Hochberg corrected p-value, as a function of sample size. The middle two panels show the values for Ed. Data were summarized using either a 0 or 16-fold difference cutoff (D). The number of features of non-o features that are identified as significantly different in the full dataset are shown in the top left corner of each plot. The bottom two panels show the effect size distribution of features identified as false positives by Ed at four different sample sizes. The dashed grey lines show the cutoff for a 10% FDR and an 80% power to detect." }

par(mfrow=c(3,2))
# effect size 1, 2 fold difference
ret.list_10 <- get.tpfp(ref.set.ty, test.set.ty, 1,0)
ret.list_11 <- get.tpfp(ref.set.ty, test.set.ty, 1,4)

plot.ret.list.q(ret.list_10, main="q<0.1, D=0")
plot.ret.list.q(ret.list_11, main="q<0.1, D=16")

plot.ret.list(ret.list_10, main="E=1, D=0")
plot.ret.list(ret.list_11, main="E=1, D=16")


plot.FP.density(ret.list_10, ref.set.ty, pos=c(1,4,6,8), main="E=1, D=0", leg=T)
plot.FP.density(ret.list_11, ref.set.ty, pos=c(1,4,6,8), main="E=1, D=2", leg=T)

```

We reproduce the analyses in a 16S rRNA gene sequencing dataset [@bian:2017]. Here we compared microbial composition of the 8-12 year old cohort (161 samples) to the young soldier cohort (211 samples) using the OTU count table published as supplement to the paper.  As before, 100 ALDEx2 instances were conducted to identify the set of OTUs that are found reproducibly. Data derived from 16S rRNA gene sequencing of communities is similar in many ways to transcriptome data, but are inherently noisier and generally sparser than transcriptome data [@fernandes:2014;@gloorAJS:2016;@gloorFrontiers:2017]. Thus, these data provide an independent test of the generality of the conclusions outlined above.

We first replicate Figure 2 of the main manuscript using $\mathcal{E}_{d} > 1 \mathrm{, and} > 1$, and the results are shown in Figure 6 and the code is in code block `R-Block-ty-fptp`. Large effect sizes are uncommon in microbiome datasets [@Falony:2016aa;@Tsilimigras:2016aa] because the dispersion is extreme as shown in Figure 7. The code for this is contained in `R-block-ty-volcano`.

The left panel in Figure 7 shows the relationship between dispersion (diff.win from ALDEx2) and relative abundance (rab.all from ALDEx2) for the yeast transcriptome and the 16S rRNA gene sequencing dataset. Tthe transcriptome dataset has a strong relationship, where the most abundant transcripts tend to have extremely low dispersion, and the rarest features (those near the low-count margin) have the greatest dispersion.  In contrast, the relationship between abundance and dispersion is nearly uncoupled in the 16S rRNA gene sequencing dataset.


```{r R-block-ty-volcano, results="hide", eval=T, echo=F, message=F, error=F, warnings=F, fig.height=5, fig.width=12, fig.cap="Abundance  vs. Dispersion, Volcano and Effect plots. The A vs D plot shows the relationship between the dispersion in the dataset vs. the relative abundance for the yeast transcriptome dataset in the main text, and the 16S rRNA gene sequencing dataset. Dispersion is the MMAD for each feature, and the Relative abundance is the median of the clr-transformed Dir Monte-Carlo instances (rab.all and diff.win from ALDEx2). The Volcano and Effect plots show features identified by Ed , adjusted p-values and absolute differences when detecting differential features between two groups for the 16S rRNA gene sequencing dataset. These plots compare the features identified by Ed and by q-scores and a 16-fold fold-change thresholds in the full dataset. This large absolute fold change is required as the within-group dispersion is enormous in this and other 16S rRNA gene sequencing datasets. In these plots all features that have a q score less than 0.1 also have an effect size greater than 1. Thus, the features in magenta are only identified as significantly different by q scores, those in orange are significantly different by both their q score and their effect size, and features in blue are significant by their q score, their effect size and their absolute difference. The dashed grey lines in the two plots demarcate the 16-fold difference location; note that the difference is in a log2 scale. The horizontal solid line in the Volcano plot indicates a FDR (q score) of 0.1. The diagonal solid lines in the Effect plot indicate the boundary where the difference equals the dispersion; ie, where the effect size is 1." }
#### Microbiome volcano and effect plots
par(mfrow=c(1,3))
plot(ref.set.ty[[1]]$diff.win, ref.set.ty[[1]]$rab.all, pch=19,
    col=rgb(1,0,0,0.2), xlim=c(0,6), ylim=c(-11,11),
    xlab="Dispersion", ylab="Relative abundance", main="A vs D")
points(ref.set.yeast[[1]]$diff.win, ref.set.yeast[[1]]$rab.all,
    pch=19, col=rgb(0,0,1,0.2))
legend(4,-4, legend=c("16S rRNA gene", "transcriptome"),
    col=c("red", "blue"), pch=19)

p.sig <- ref.set.ty[[1]]$we.eBH < 0.1
e.sig <- abs(ref.set.ty[[1]]$effect) > 1
both.sig <- abs(ref.set.ty[[1]]$effect) > 1 & abs(ref.set.ty[[1]]$diff.btw) > 4

# volcano
plot(ref.set.ty[[1]]$diff.btw, log10(ref.set.ty[[1]]$we.eBH) * -1,
    pch=19, cex=0.8, col=rgb(0,0,0,0.1), main="Volcano",
    xlab="Difference", ylab="-1 * log(q)")
points(ref.set.ty[[1]]$diff.btw[p.sig], log10(ref.set.ty[[1]]$we.eBH[p.sig]) * -1, pch=19, cex=0.8, col=rgb(1,0,1,0.3))
points(ref.set.ty[[1]]$diff.btw[e.sig], log10(ref.set.ty[[1]]$we.eBH[e.sig]) * -1, pch=19, cex=0.8, col=rgb(0.95,0.5,0,0.3))
points(ref.set.ty[[1]]$diff.btw[both.sig], log10(ref.set.ty[[1]]$we.eBH[both.sig]) * -1, pch=19, cex=0.8, col=rgb(0,0,1,0.3))
abline(h=log10(0.1) *-1, col="grey", lwd=2)
abline(v=-4, col="grey", lty=2, lwd=2)
abline(v=4, col="grey", lty=2, lwd=2)
legend(-1,90, legend=c("not different", "q < 0.1", "E > 1", "E > 1, D>32"), pch=19,    col=c(col=rgb(0,0,0,0.5),rgb(1,0,1,0.5),col=rgb(0.95,0.5,0,0.5),col=rgb(0,0,1,0.5)))

# t(effect)
plot( ref.set.ty[[1]]$diff.win, ref.set.ty[[1]]$diff.btw,pch=19, cex=0.8, col=rgb(0,0,0,0.1),
    main="Effect",
    xlab="Dispersion", ylab="Difference")
points( ref.set.ty[[1]]$diff.win[p.sig], ref.set.ty[[1]]$diff.btw[p.sig],pch=19, cex=0.8, col=rgb(1,0,1,0.3))
points( ref.set.ty[[1]]$diff.win[e.sig], ref.set.ty[[1]]$diff.btw[e.sig],pch=19, cex=0.8, col=rgb(0.95,0.5,0,0.3))
points( ref.set.ty[[1]]$diff.win[both.sig], ref.set.ty[[1]]$diff.btw[both.sig],pch=19, cex=0.8, col=rgb(0,0,1,0.5))
abline()
abline(h=-4, col="grey", lty=2, lwd=2)
abline(h=4, col="grey", lty=2, lwd=2)
abline(0,1, col="grey", lwd=2)
abline(0,-1, col="grey", lwd=2)
```

\clearpage

Finally, `R-block-how-many` and Figure 8 shows the TP-FP characteristics of the two real datasets and the simulated datasets. For this we superimposed the 99th percentile of effect sizes from subsets of the 16S rRNA gene sequencing dataset or form the transcriptome dataset where comparison groups were randomly sampled from the same group. Thus, this analysis accounts for biological and sampling variation, and the simulated datasets include only sampling variation. The $\mathcal{E}_{d}$ values for the randomly sub-sampled data were computed in two ways. First, using the clr-transformed point estimates of the data, and second using the clr-transformed Bayesian posterior distributions of the data. No difference between group was expected. We see that the point estimate values from the real data overlap almost exactly with the point estimate values from the simulated data, indicating that the simulated data is a good proxy for the actual biological data. Interestingly, the 99th percentile of $\mathcal{E}_{d}$ derived from the expected value of the Bayesian posterior estimate is significantly more conservative than the point-estimate derived statistic. In this case, $\mathcal{E}_{d}$ values of 2 or more are likely reasonable regardless of the sample size or dataset, and using a cutoff of $\mathcal{E}_{d}$ of 1 is likely to be a safe threshold as long as the sample size per group is at least 10.


```{r R-block-how-many, echo=F, eval=T, fig.height=7, fig.width=10, fig.cap="The proportion of features that have an effect size $\\mathcal{E}_{d}$ as a function of per-group sample size when there is known to be no true effect. The line graphs show plots of the same data as in Figure 1; these are the 99th percentile effect sizes from Normal, Cauchy, Uniform and Beta random distributions. The boxplots overlay the 99th percentile effect sizes from subsets of the 16S rRNA gene sequencing dataset or the transcriptome dataset where groups were randomly sampled from the same group, and thus no difference between group was expected. Sample sizes were chosen up to a maximum of one half the total number of samples per group. The plots show boxplots of the 99th percentile of the effect size of 100 subsets per sample size. "}

load("data/null.set.effect.Rdata")
load("data/null.set.ty.effect.Rdata")
load("data/null.set.ty.single.effect.RData")
load("data/list.null.Rdata")

seq.test <- c(2,3,5,10,15,20,30,50,100)

min.quant.ty <- matrix(data=NA, nrow=100,ncol=9)
max.quant.ty <- matrix(data=NA, nrow=100,ncol=9)

min.quant.pt.ty <- matrix(data=NA, nrow=100,ncol=9)
max.quant.pt.ty <- matrix(data=NA, nrow=100,ncol=9)

min.quant <- matrix(data=NA, nrow=100,ncol=9)
max.quant <- matrix(data=NA, nrow=100,ncol=9)

for(i in 1:9){
    for(j in 1:100){
        min.quant.ty[j,i] <- apply(null.set.ty.effect[[i]][[j]], 2, function(x) quantile(x, probs=c(0.005, 0.995)))[11]
        max.quant.ty[j,i] <- apply(null.set.ty.effect[[i]][[j]], 2, function(x) quantile(x, probs=c(0.005, 0.995)))[12]

        min.quant.pt.ty[j,i] <- quantile(null.set.ty.single.effect[[i]][[j]], probs=c(0.005, 0.995))[1]
        max.quant.pt.ty[j,i] <- quantile(null.set.ty.single.effect[[i]][[j]], probs=c(0.005, 0.995))[2]

    }
}

for(i in 1:6){
    for(j in 1:100){
        min.quant[j,i] <- apply(null.set.effect[[i]][[j]], 2, function(x) quantile(x, probs=c(0.005, 0.995)))[11]
        max.quant[j,i] <- apply(null.set.effect[[i]][[j]], 2, function(x) quantile(x, probs=c(0.005, 0.995)))[12]
    }
}

par(mfrow=c(1,1))
plot(1, type="n", xlim=c(2,40), ylim=c(0,10), xlab="sample size per group", ylab=expression("d"[NEF]), log="x")

boxplot(max.quant.pt.ty[,1], boxwex=0.2, add=T, at=2, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,2], boxwex=0.2, add=T, at=3, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,3], boxwex=0.2, add=T, at=5, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,4], boxwex=0.2, add=T, at=10, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,5], boxwex=0.2, add=T, at=15, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,6], boxwex=0.2, add=T, at=20, border=rgb(0,0,0,0.6), outline=F)
boxplot(max.quant.pt.ty[,7], boxwex=0.2, add=T, at=30, border=rgb(0,0,0,0.6), outline=F)

boxplot(max.quant[,1], boxwex=0.2, add=T, at=1.9, border=rgb(0,0,1,0.4), outline=F)
boxplot(max.quant[,2], boxwex=0.2, add=T, at=2.85, border=rgb(0,0,1,0.4), outline=F)
boxplot(max.quant[,3], boxwex=0.2, add=T, at=4.75, border=rgb(0,0,1,0.4), outline=F)
boxplot(max.quant[,4], boxwex=0.2, add=T, at=9.5, border=rgb(0,0,1,0.4), outline=F)
boxplot(max.quant[,5], boxwex=0.2, add=T, at=14.25, border=rgb(0,0,1,0.4), outline=F)
boxplot(max.quant[,6], boxwex=0.2, add=T, at=19, border=rgb(0,0,1,0.4), outline=F)

boxplot(max.quant.ty[,1], boxwex=0.2, add=T, at=2.1, border=rgb(1,0,0,0.4), outline=F)
boxplot(max.quant.ty[,2], boxwex=0.2, add=T, at=3.15, border=rgb(1,0,0,0.4), outline=F)
boxplot(max.quant.ty[,3], boxwex=0.2, add=T, at=5.25, border=rgb(1,0,0,0.4), outline=F)
boxplot(max.quant.ty[,4], boxwex=0.2, add=T, at=10.5, border=rgb(1,0,0,0.4), outline=F)
boxplot(max.quant.ty[,5], boxwex=0.2, add=T, at=15.75, border=rgb(1,0,0,0.4), outline=F)
boxplot(max.quant.ty[,6], boxwex=0.2, add=T, at=21, border=rgb(1,0,0,0.4), outline=F)


 points(list.null$rnorm.null.effect[,1],list.null$rnorm.null.effect[,8], ylim=c(0,2), type="l")
 points(list.null$runif.null.effect[,1],list.null$runif.null.effect[,8], ylim=c(0,5), type="l", col="red")
 points(list.null$rbeta.null.effect[,1],list.null$rbeta.null.effect[,8], ylim=c(0,5), type="l", col="cyan")
 points(list.null$rcauchy.null.effect[,1],list.null$rcauchy.null.effect[,8], ylim=c(0,5), type="l", col="orange")

legend(10,8, legend=c("16S point","16S posterior","RNA-seq posterior"), col=c(rgb(0,0,0,0.6),rgb(1,0,0,0.4),rgb(0,0,1,0.4)), pch=0)
```

\clearpage
# References
